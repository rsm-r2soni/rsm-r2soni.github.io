[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishabh Soni",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nYour Name\n\n\nMay 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nYour Name\n\n\nMay 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nYour Name\n\n\nMay 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "HW1/hw1_questions.html",
    "href": "HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/hw1_questions.html#introduction",
    "href": "HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/hw1_questions.html#data",
    "href": "HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "HW1/hw1_questions.html#experimental-results",
    "href": "HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "HW1/hw1_questions.html#simulation-experiment",
    "href": "HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project2/hw1_questions.html",
    "href": "projects/project2/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe authors conducted a large-scale natural field experiment by mailing over 50,000 fundraising letters to previous donors of a politically active nonprofit organization. Individuals were randomly assigned to a control group or one of several treatment groups. The treatment groups received letters offering a matching grant that varied in size (e.g., $25k, $50k, $100k), matching ratio ($1:$1, $2:$1, $3:$1), and suggested donation amount (based on their highest previous contribution).\nThis experiment allowed the researchers to isolate the causal effects of financial incentives on both the probability of donating and the amount donated. Their goal was to understand whether increasing the effective “value” of a donation would motivate more people to give — and whether bigger matches further increased this effect.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#introduction",
    "href": "projects/project2/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe authors conducted a large-scale natural field experiment by mailing over 50,000 fundraising letters to previous donors of a politically active nonprofit organization. Individuals were randomly assigned to a control group or one of several treatment groups. The treatment groups received letters offering a matching grant that varied in size (e.g., $25k, $50k, $100k), matching ratio ($1:$1, $2:$1, $3:$1), and suggested donation amount (based on their highest previous contribution).\nThis experiment allowed the researchers to isolate the causal effects of financial incentives on both the probability of donating and the amount donated. Their goal was to understand whether increasing the effective “value” of a donation would motivate more people to give — and whether bigger matches further increased this effect.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#data",
    "href": "projects/project2/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nlibrary(haven) \n\ndf &lt;- read_dta(\"/home/jovyan/Desktop/Marketing Analytics/projects/project2/karlan_list_2007.dta\") \n\n\n\nReading the Data\nWe use the dataset made available by the authors, which contains over 50,000 observations corresponding to individual donors who received various fundraising letter treatments.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo evaluate whether random assignment was successful, we begin by testing whether the number of months since the last donation (mrm2) differs across treatment and control groups.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndf_clean &lt;- df %&gt;% filter(!is.na(mrm2))\n\n\n\nT-test\n\nlibrary(broom) \n\nWarning: package 'broom' was built under R version 4.4.3\n\ntidy(t.test(mrm2 ~ treatment, data = df_clean))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  -0.0137      13.0      13.0    -0.120   0.905    33394.   -0.238     0.211\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\n\nLinear regression\n\nsummary(lm(mrm2 ~ treatment, data = df_clean)) \n\n\nCall:\nlm(formula = mrm2 ~ treatment, data = df_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.012  -9.012  -5.012   6.002 154.988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.99814    0.09353 138.979   &lt;2e-16 ***\ntreatment    0.01369    0.11453   0.119    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.08 on 50080 degrees of freedom\nMultiple R-squared:  2.851e-07, Adjusted R-squared:  -1.968e-05 \nF-statistic: 0.01428 on 1 and 50080 DF,  p-value: 0.9049\n\n\nBoth the t-test and the regression confirm that there is no statistically significant difference in the number of months since last donation (mrm2) between the treatment and control groups.\nThis supports the validity of the experimental design and aligns with Table 1 in Karlan & List (2007), which also shows balanced covariates. These results justify interpreting treatment effects causally in subsequent analyses."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#experimental-results",
    "href": "projects/project2/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nWe begin by visualizing the proportion of people who donated in each group, followed by statistical testing using a t-test, linear regression, and a probit model to confirm the results shown in Tables 2a and 3 of Karlan & List (2007).\nThis shows the percentage of people who donated by treatment group.\n\nlibrary(ggplot2) \n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\ndf %&gt;% group_by(treatment) %&gt;% summarise(response_rate = mean(gave)) %&gt;% ggplot(aes(x = factor(treatment), y = response_rate, fill = factor(treatment))) + geom_bar(stat = \"identity\") + labs(x = \"Group\", y = \"Proportion Donated\", title = \"Donation Rate by Treatment\") + theme_minimal() \n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.) t-test ::: {.cell}\nlibrary(broom) \n\ntidy(t.test(gave ~ treatment, data = df)) \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00418    0.0179    0.0220     -3.21 0.00133    36577. -0.00673  -0.00163\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n::: Linear Regression\n\nsummary(lm(gave ~ treatment, data = df)) \n\n\nCall:\nlm(formula = gave ~ treatment, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nlibrary(margins) \nprobit_model &lt;- glm( gave ~ treatment, data = df, family = binomial(link = \"probit\") ) \n\nThe results show that the treatment group had a significantly higher probability of donating compared to the control group. This finding is confirmed through the t-test, linear regression, and probit model, all of which show a positive and statistically significant treatment effect.\nThese results replicate the patterns seen in Tables 2a and 3 of Karlan & List (2007). They support the conclusion that matching donations are an effective nudge for charitable giving, likely by increasing the perceived impact of a donor’s contribution.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nTo assess whether larger match ratios increase the likelihood of donation, I compare donation rates across 1:1, 2:1, and 3:1 match conditions using both t-tests and regression models.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8? ::: {.cell}\ntreat_only &lt;- df %&gt;% filter(treatment == 1) \n\nt1_vs_2 &lt;- t.test(gave ~ ratio2, data = treat_only %&gt;% filter(ratio3 == 0))\n\nt1_vs_3 &lt;- t.test(gave ~ ratio3, data = treat_only %&gt;% filter(ratio2 == 0)) \n\ntidy(t1_vs_2)\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00188    0.0207    0.0226    -0.965   0.335    22225. -0.00571   0.00194\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(t1_vs_3) \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00198    0.0207    0.0227     -1.02   0.310    22215. -0.00582   0.00185\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n::: ### Interpretation\nThe t-tests compare donation response rates among treatment group members who received different match ratios: 1:1, 2:1, and 3:1. The results show that the differences in response rates between 1:1 and 2:1, and between 1:1 and 3:1, are not statistically significant at the 5% level.\nThis indicates that while being offered a matching donation does increase the likelihood of giving (as shown earlier), increasing the match from 1:1 to 2:1 or 3:1 does not further increase the likelihood of donating. These findings support the conclusion in Karlan & List (2007) that larger match ratios had no additional impact on participation.\nIn short, donors are responsive to the presence of a match, but not to how generous it is.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\ndf &lt;- df %&gt;% \nmutate(ratio1 = ifelse(ratio2 == 0 & ratio3 == 0, 1, 0)) \ntreat_df &lt;- df %&gt;% filter(treatment == 1) \nmodel_ratios &lt;- lm(gave ~ ratio1 + ratio2 + ratio3, data = treat_df) \ntidy(model_ratios)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  0.0227     0.00139   16.3     9.44e-60\n2 ratio1      -0.00198    0.00197   -1.01    3.13e- 1\n3 ratio2      -0.000100   0.00197   -0.0508  9.59e- 1\n4 ratio3      NA         NA         NA      NA       \n\n\n\n\nInterpretation\nThe regression estimates the difference in donation probability across different match ratios using ratio1 (1:1), ratio2 (2:1), and ratio3 (3:1) as indicator variables. The omitted category (baseline) is ratio1, so the coefficients on ratio2 and ratio3 represent the difference relative to the 1:1 match.\nThe results show that neither ratio2 nor ratio3 has a statistically significant effect on the probability of donating compared to ratio1. This means that increasing the match from 1:1 to 2:1 or 3:1 does not significantly boost the likelihood of donation.\nThese findings align with the authors’ conclusion that larger match ratios did not have additional impact — it’s the presence of a match that matters more than how generous it is.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations? ::: {.cell}\nmatch_rates &lt;- df %&gt;%\nfilter(treatment == 1) %&gt;%\ngroup_by(ratio) %&gt;%\nsummarise(response_rate = mean(gave, na.rm = TRUE))\n\nmatch_rates \n\n# A tibble: 3 × 2\n  ratio     response_rate\n  &lt;dbl+lbl&gt;         &lt;dbl&gt;\n1 1                0.0207\n2 2                0.0226\n3 3                0.0227\n\n:::\n\ndiff_2_1_vs_1_1 &lt;- match_rates$response_rate[match_rates$ratio == 2] -\n                   match_rates$response_rate[match_rates$ratio == 1]\n\ndiff_3_1_vs_2_1 &lt;- match_rates$response_rate[match_rates$ratio == 3] -\n                   match_rates$response_rate[match_rates$ratio == 2]\n\ndiff_2_1_vs_1_1\n\n[1] 0.001884251\n\ndiff_3_1_vs_2_1\n\n[1] 0.000100024\n\n\n\ncoef_diff_2_1_vs_1_1 &lt;- coef(model_ratios)[\"ratio2\"] - coef(model_ratios)[\"ratio1\"]\ncoef_diff_3_1_vs_2_1 &lt;- coef(model_ratios)[\"ratio3\"] - coef(model_ratios)[\"ratio2\"]\n\ncoef_diff_2_1_vs_1_1\n\n     ratio2 \n0.001884251 \n\ncoef_diff_3_1_vs_2_1\n\nratio3 \n    NA \n\n\n\n\nInterpretation\nFrom the direct data comparison:\n\nThe increase in donation rate from a 1:1 to 2:1 match is approximately 0.00188 (or 0.188 percentage points)\nThe increase from 2:1 to 3:1 is only 0.00010 (or 0.01 percentage points)\n\nFrom the regression model:\n\nThe difference between the estimated effects of 2:1 and 1:1 is also 0.00188, confirming consistency with the observed data\nThe difference between 3:1 and 2:1 could not be calculated (NA), likely because the 3:1 dummy variable was excluded due to perfect multicollinearity or zero variance\n\nOverall, these results suggest that increasing the match ratio from 1:1 to 2:1 has a minimal impact, and increasing it from 2:1 to 3:1 has virtually no additional effect.\nThis supports Karlan & List’s claim that larger match ratios do not meaningfully boost participation — donors are motivated by the presence of a match, but not by how large it is.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\ntidy(t.test(amount ~ treatment, data = df)) \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   -0.154     0.813     0.967     -1.92  0.0551    36216.   -0.311   0.00334\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\n\nSize of Charitable Contribution (Conditional on Donating)\nNext, we restrict the data to only those who actually donated (gave == 1) to see if treatment affected how much people gave.\n\ndf_donors &lt;- df %&gt;% filter(gave == 1) \ntidy(t.test(amount ~ treatment, data = df_donors)) \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     1.67      45.5      43.9     0.585   0.559      557.    -3.94      7.27\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\nlibrary(ggplot2) \nlibrary(dplyr) \ndf_donors &lt;- df_donors %&gt;% mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\")) \nmean_donations &lt;- df_donors %&gt;% \ngroup_by(group) %&gt;% \nsummarise(avg = mean(amount), .groups = \"drop\") \ndf_donors &lt;- df_donors %&gt;% \n    left_join(mean_donations, by = \"group\") \nggplot(df_donors, aes(x = amount)) + \ngeom_histogram( binwidth = 5, aes(fill = group), color = \"black\", alpha = 0.7 ) + \ngeom_vline(aes(xintercept = avg), color = \"red\", linetype = \"dashed\", linewidth = 1) + \nfacet_wrap(~group, ncol = 2) + \nscale_fill_manual(values = c(\"Treatment\" = \"#69b3a2\", \"Control\" = \"#f8766d\")) + \nlabs( title = \"Distribution of Donation Amounts Among Donors\", x = \"Donation Amount\", y = \"Number of Donors\" ) + \ntheme_minimal() + \ntheme( plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14), strip.text = element_text(size = 12, face = \"bold\") ) \n\n\n\n\n\n\n\n\n\n\nInterpretation\nWhen considering all individuals (including non-donors), the treatment group gave slightly more on average, but the difference is not statistically significant. This suggests that offering a match may slightly raise the total dollars raised per person, but the effect is modest when averaged over the full sample.\nWhen we restrict to only those who actually donated, we again see that the average donation size is similar across groups. The regression coefficient on treatment is small and statistically insignificant.\nThe histogram confirms that the donation distributions are quite similar between the treatment and control groups. Most donations cluster around the same range, and the average donation (marked with a red dashed line) is nearly identical across conditions.\nConclusion: The main effect of the treatment was to increase the number of donors — not the amount given by each donor. While total revenue increases with treatment, it’s driven by more people giving, not larger individual donations."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#simulation-experiment",
    "href": "projects/project2/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo better understand the behavior of the t-statistic, we use simulation to illustrate the Law of Large Numbers (LLN). This shows how repeated random samples can help us estimate the true mean difference between treatment and control groups.\nWe simulate: - Control group: Bernoulli(p = 0.018) - Treatment group: Bernoulli(p = 0.022) - We draw 100,000 values and plot the running average of the difference\n\nset.seed(123) \ncontrol_sim &lt;- rbinom(100000, 1, 0.018) \ntreat_sim &lt;- rbinom(100000, 1, 0.022) \ndiffs &lt;- treat_sim - control_sim[1:100000] \ncum_avg &lt;- cumsum(diffs) / seq_along(diffs) \nlibrary(ggplot2) \ndf_sim &lt;- data.frame( Simulations = 1:100000, CumulativeAverage = cum_avg ) \nggplot(df_sim, aes(x = Simulations)) + \ngeom_line(aes(y = CumulativeAverage, color = \"Observed Average\"), linewidth = 1) + \ngeom_hline(aes(yintercept = 0.004, color = \"True Mean Difference\"), linetype = \"dashed\", linewidth = 1) + \nscale_color_manual( values = c(\"Observed Average\" = \"#0073C2\", \"True Mean Difference\" = \"#D7263D\") ) + \nlabs( title = \"Law of Large Numbers: Convergence of Mean Difference\", x = \"Number of Simulations\", y = \"Cumulative Average Difference\" ) + \ntheme_minimal(base_size = 14) + \ntheme( plot.title = element_text(hjust = 0.5, face = \"bold\"), legend.position = \"top\" )\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe graph above demonstrates that as the number of simulated respondents grows, the average difference in donation rates between groups stabilizes near the true difference of 0.004. The blue line represents the cumulative average, and the red dashed line shows the expected population-level difference.\nThis simulation illustrates the Law of Large Numbers in action: with more data points, our estimate of the mean becomes increasingly accurate. It also reinforces the intuition behind statistical tests — they’re more reliable when sample sizes are large because the estimates converge to the truth.\n\n\nCentral Limit Theorem\nTo demonstrate how the sampling distribution of the mean behaves with different sample sizes, we simulate average donation rate differences for the control and treatment groups. We simulate 1,000 experiments for each sample size and visualize the resulting sampling distributions.\n\nset.seed(42)\n\nsample_sizes &lt;- c(50, 200, 500, 1000)\n\ntrue_diff &lt;- 0.022 - 0.018\n\npar(mfrow = c(2, 2))\n\nfor (n in sample_sizes) {\n    diffs &lt;- replicate(1000, {\n    control &lt;- rbinom(n, 1, 0.018)\n    treatment &lt;- rbinom(n, 1, 0.022)\n    mean(treatment) - mean(control)\n})\n\nhist(diffs,\n    breaks = 30,\n    col = \"#A6CEE3\",\n    border = \"#1F78B4\",\n    main = paste(\"Sample Size:\", n),\n    xlab = \"Average (Treatment - Control)\")\n\nabline(v = 0, col = \"black\", lty = 2)\nabline(v = true_diff, col = \"#E31A1C\", lty = 2)\n\nlegend(\"topright\",\n        legend = sprintf(\"True Diff = %.4f\", true_diff),\n        col = \"#E31A1C\",\n        lty = 2,\n        bty = \"n\")\n}\n\n\n\n\n\n\n\n\n\n\nInterpretation\nEach histogram above shows the sampling distribution of the difference in average donation rates for a given sample size, using 1,000 repeated simulations.\n\nFor n = 50, the distribution is wide and irregular, with many simulations near zero, showing high variability.\nAs sample sizes increase (n = 200, 500), the distribution begins to resemble a bell shape and centers more consistently around the true mean difference.\nBy n = 1000, the shape is sharply peaked and tightly centered around the expected value of 0.004.\n\nThis simulation confirms the Central Limit Theorem: even though the underlying data are binary, the distribution of the sample mean becomes more normal and stable as the number of observations grows. The takeaway is clear — with enough data, our estimates become far more reliable."
  },
  {
    "objectID": "projects/project1/hw2_questions.html",
    "href": "projects/project1/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndf[\"iscustomer\"] = df[\"iscustomer\"].astype(str)\ndf[\"patents\"] = pd.to_numeric(df[\"patents\"], errors=\"coerce\")\n\nplot_df = df[[\"iscustomer\", \"patents\"]].dropna()\n\nplt.figure(figsize=(8, 5))\nsns.histplot(data=plot_df, x=\"patents\", hue=\"iscustomer\", kde=True, element=\"step\", stat=\"density\")\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.tight_layout()\nplt.show()\n\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\nThe histogram below compares the distribution of the number of patents between firms that use Blueprinty’s software (iscustomer = 1) and those that do not (iscustomer = 0).\nWe observe that: - Non-customers tend to have 2–4 patents, with a sharper peak and tighter spread. - Customers show a flatter distribution with slightly more density at higher patent counts (6+), suggesting they may be more likely to have more patents.\nHowever, Blueprinty customers are not selected at random. It is possible that other factors — such as a firm’s age or region — differ systematically between customers and non-customers. These differences may help explain the observed variation in patent output, and should be accounted for before drawing causal conclusions.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nTo assess whether Blueprinty customers differ systematically from non-customers, we compare firm ages and regional distribution by customer status.\nUnderstanding these differences is crucial, as they may confound the observed relationship between Blueprinty usage and patent output.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n\n\nage_df = df[[\"iscustomer\", \"age\"]].dropna()\n\n\nplt.figure(figsize=(7, 5))\nsns.boxplot(x=\"iscustomer\", y=\"age\", data=age_df)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (years)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\")\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Firm Count\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFrom the plots above, we observe the following:\n\nAge Differences:\nThe median age of Blueprinty customers (iscustomer = 1) is slightly higher than that of non-customers. The interquartile ranges also suggest that customer firms tend to be slightly older on average, though both groups have overlapping age distributions. This indicates that age may partially explain differences in patent productivity, and should be controlled for in modeling.\nRegional Differences:\nBlueprinty customers are not evenly distributed across regions. In particular, the Northeast has a much higher proportion of customers compared to other regions, where non-customers dominate. This suggests a strong regional bias in Blueprinty’s customer base, which may reflect regional innovation ecosystems or Blueprinty’s sales efforts.\n\nThese systematic differences in age and region reinforce the need to control for potential confounders when estimating the effect of Blueprinty usage on patenting outcomes.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\n\n\nLet \\(Y_i\\) be the number of patents awarded to firm \\(i\\), where \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\). The probability mass function of the Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across \\(n\\) firms, the likelihood function for the full sample is the product of individual probabilities:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nTaking the natural logarithm of the likelihood gives us the log-likelihood function:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda_i + Y_i \\log \\lambda_i - \\log Y_i! \\right)\n\\]\nThis log-likelihood is what we maximize when estimating a Poisson regression model.\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_log_likelihood(lmbda, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model.\n\n    Parameters:\n    - lmbda: array-like of Poisson rates (λ_i)\n    - Y: array-like of observed counts (Y_i)\n\n    Returns:\n    - total log-likelihood (float)\n    \"\"\"\n    lmbda = np.asarray(lmbda)\n    Y = np.asarray(Y)\n\n    \n    log_lik = -lmbda + Y * np.log(lmbda) - gammaln(Y + 1)\n    return np.sum(log_lik)\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nY = df[\"patents\"].dropna().astype(float).values\n\n\nlambda_vals = np.linspace(0.1, 10, 100)\n\n\nlog_liks = [poisson_log_likelihood(np.full_like(Y, lam), Y) for lam in lambda_vals]\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks, label=\"Log-Likelihood\", color='blue')\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe plot above shows how the Poisson log-likelihood varies as we change the constant value of (), using the observed patent counts as the outcome.\nWe observe that:\n\nThe log-likelihood increases initially with (), reaches a maximum around (), and then begins to decline.\nThis peak represents the maximum likelihood estimate (MLE) for () when assuming a constant rate across all firms.\nThe shape of the curve confirms the expected behavior of the Poisson likelihood — it’s unimodal, with a clear optimal point that balances the trade-off between under- and over-predicting patent counts.\n\nThis exercise illustrates the core principle of MLE: choosing the parameter value that makes the observed data most likely under the assumed statistical model.\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\n\n\n\nTo find the maximum likelihood estimate (MLE) of (), we begin with the log-likelihood function for a Poisson model:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nTo find the MLE, we take the derivative with respect to () and set it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\]\nSimplifying:\n\\[\n\\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\quad \\Rightarrow \\quad\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ():\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = n\n\\quad \\Rightarrow \\quad\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThus, the MLE of () is simply the sample mean ({Y}), which aligns with our intuition since the Poisson distribution has mean equal to ().\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\nY = df[\"patents\"].dropna().astype(float).values\n\n\ndef neg_log_likelihood(lam_array):\n    lam = lam_array[0]\n    if lam &lt;= 0:\n        return np.inf  \n    return -poisson_log_likelihood(np.full_like(Y, lam), Y)\n\n\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(1e-6, None)])\n\n\nlambda_mle = result.x[0]\nlambda_mle\n\nnp.float64(3.6846662953477973)\n\n\n\n\n\nUsing numerical optimization via scipy.optimize.minimize(), we estimated the Poisson rate parameter () that maximizes the log-likelihood of the observed data.\nThe optimizer returned:\n[ _{} ]\nThis estimate matches the sample mean of the observed patent counts, confirming the analytical result derived earlier — that the MLE for () in a Poisson distribution is the average of the data:\n[ _{} = {Y} ]\nThis result strengthens our understanding of maximum likelihood estimation and provides a solid baseline before incorporating covariates in a full regression model.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nfrom scipy.special import gammaln\nimport numpy as np\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    \"\"\"\n    Computes the log-likelihood for a Poisson regression model\n    where lambda_i = exp(X_i' * beta).\n    \n    Parameters:\n    - beta: Coefficient vector (length p)\n    - Y: Observed counts (length n)\n    - X: Covariate matrix (n x p)\n\n    Returns:\n    - Total log-likelihood (scalar)\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    X = np.asarray(X, dtype=float)\n\n    lambda_i = np.exp(X @ beta)\n    log_lik = -lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1)\n    return np.sum(log_lik)\n\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport numdifftools as nd\n\n\ndf = pd.read_csv(\"blueprinty.csv\").dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"])\n\n\ndf[\"age_centered\"] = df[\"age\"] - df[\"age\"].mean()\ndf[\"age_sq\"] = df[\"age_centered\"] ** 2\n\n\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_centered\", \"age_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.astype(float).values\nY = df[\"patents\"].astype(float).values\n\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    lambda_i = np.exp(X @ beta)\n    return np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n\n\ndef neg_log_likelihood_beta(beta):\n    return -poisson_regression_log_likelihood(beta, Y, X_matrix)\n\n\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood_beta, init_beta, method=\"BFGS\")\n\n\nbeta_mle = result.x\n\n\nhessian_fun = nd.Hessian(neg_log_likelihood_beta)\nhessian_matrix = hessian_fun(beta_mle)\ncov_matrix = np.linalg.inv(hessian_matrix)\nse_beta = np.sqrt(np.diag(cov_matrix))\n\n\nresults_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": se_beta\n})\n\nresults_table.round(4)\n\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: divide by zero encountered in matmul\n  lambda_i = np.exp(X @ beta)\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: overflow encountered in matmul\n  lambda_i = np.exp(X @ beta)\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: invalid value encountered in matmul\n  lambda_i = np.exp(X @ beta)\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: overflow encountered in matmul\n  lambda_i = np.exp(X @ beta)\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.3447\n0.0383\n\n\n1\nage_centered\n-0.0080\n0.0021\n\n\n2\nage_sq\n-0.0030\n0.0003\n\n\n3\niscustomer\n0.2076\n0.0309\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n\n\n6\nregion_South\n0.0566\n0.0527\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n\nX_sm = sm.add_constant(X.drop(columns=\"intercept\")).astype(float)\nY_sm = Y.astype(float)\n\n\nmodel = sm.GLM(Y_sm, X_sm, family=sm.families.Poisson())\nresults = model.fit()\n\n\nsm_results_table = pd.DataFrame({\n    \"Variable\": results.params.index,\n    \"Coefficient (sm.GLM)\": results.params.values,\n    \"Std. Error (sm.GLM)\": results.bse.values\n})\n\nsm_results_table.round(4)\n\n\n\n\n\n\n\n\nVariable\nCoefficient (sm.GLM)\nStd. Error (sm.GLM)\n\n\n\n\n0\nconst\n1.3447\n0.0384\n\n\n1\nage_centered\n-0.0080\n0.0021\n\n\n2\nage_sq\n-0.0030\n0.0003\n\n\n3\niscustomer\n0.2076\n0.0309\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n\n\n6\nregion_South\n0.0566\n0.0527\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n\n\n\n\n\n\n\n\n\n\nThe Poisson regression estimates the relationship between firm characteristics and the number of patents awarded. Key interpretations include:\n\nIntercept (const = 1.3447): This is the expected log count of patents for a non-customer firm with average age and located in the baseline region (the region omitted from dummy encoding). Exponentiating gives an expected count of ((1.3447) ) patents.\nCustomer Status (iscustomer = 0.2076, SE = 0.0309): This coefficient is statistically significant and positive, indicating that Blueprinty customers are associated with a 23.1% higher expected number of patents compared to non-customers, holding all else constant.\n[ (0.2076) ]\nFirm Age Effects:\n\nThe negative coefficient on age_centered (-0.0080) suggests that, near the average, older firms have a slightly lower expected number of patents.\nThe negative coefficient on age_sq (-0.0030) implies diminishing returns or a concave relationship: patenting productivity decreases faster at higher ages.\n\nRegional Effects: The region coefficients are relatively small, and most are not statistically distinguishable from zero given their standard errors. This suggests that regional location has limited independent effect on patenting after accounting for other covariates.\n\nOverall, the model supports the conclusion that Blueprinty customers tend to be more successful in securing patents, even after controlling for firm age and region.\n\nX_0_mat = X_sm.copy()\nX_1_mat = X_sm.copy()\n\n\nX_0_mat[\"iscustomer\"] = 0\nX_1_mat[\"iscustomer\"] = 1\n\n\ny_pred_0 = results.predict(X_0_mat)\ny_pred_1 = results.predict(X_1_mat)\n\n\ndelta_y = y_pred_1 - y_pred_0\navg_treatment_effect = np.mean(delta_y)\n\navg_treatment_effect\n\nnp.float64(0.7927680710453266)\n\n\n\n\n\nTo estimate the real-world impact of Blueprinty’s software on patenting success, we conducted a counterfactual analysis. Specifically, we predicted the number of patents each firm would file under two hypothetical scenarios:\n\nScenario 1 (X_0): All firms are non-customers (iscustomer = 0)\nScenario 2 (X_1): All firms are customers (iscustomer = 1)\n\nWe then computed the difference in predicted patents for each firm and averaged those differences. The result:\n[ = {Y}{} - {Y}{} = 0.793 ]\nThis means that, on average, Blueprinty customers are expected to file approximately 0.79 more patents than they would have if they weren’t customers, holding age and region constant.\nThis provides strong evidence that using Blueprinty’s software is associated with a meaningful improvement in patenting outcomes."
  },
  {
    "objectID": "projects/project1/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project1/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndf[\"iscustomer\"] = df[\"iscustomer\"].astype(str)\ndf[\"patents\"] = pd.to_numeric(df[\"patents\"], errors=\"coerce\")\n\nplot_df = df[[\"iscustomer\", \"patents\"]].dropna()\n\nplt.figure(figsize=(8, 5))\nsns.histplot(data=plot_df, x=\"patents\", hue=\"iscustomer\", kde=True, element=\"step\", stat=\"density\")\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.tight_layout()\nplt.show()\n\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\nThe histogram below compares the distribution of the number of patents between firms that use Blueprinty’s software (iscustomer = 1) and those that do not (iscustomer = 0).\nWe observe that: - Non-customers tend to have 2–4 patents, with a sharper peak and tighter spread. - Customers show a flatter distribution with slightly more density at higher patent counts (6+), suggesting they may be more likely to have more patents.\nHowever, Blueprinty customers are not selected at random. It is possible that other factors — such as a firm’s age or region — differ systematically between customers and non-customers. These differences may help explain the observed variation in patent output, and should be accounted for before drawing causal conclusions.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nTo assess whether Blueprinty customers differ systematically from non-customers, we compare firm ages and regional distribution by customer status.\nUnderstanding these differences is crucial, as they may confound the observed relationship between Blueprinty usage and patent output.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n\n\nage_df = df[[\"iscustomer\", \"age\"]].dropna()\n\n\nplt.figure(figsize=(7, 5))\nsns.boxplot(x=\"iscustomer\", y=\"age\", data=age_df)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (years)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\")\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Firm Count\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFrom the plots above, we observe the following:\n\nAge Differences:\nThe median age of Blueprinty customers (iscustomer = 1) is slightly higher than that of non-customers. The interquartile ranges also suggest that customer firms tend to be slightly older on average, though both groups have overlapping age distributions. This indicates that age may partially explain differences in patent productivity, and should be controlled for in modeling.\nRegional Differences:\nBlueprinty customers are not evenly distributed across regions. In particular, the Northeast has a much higher proportion of customers compared to other regions, where non-customers dominate. This suggests a strong regional bias in Blueprinty’s customer base, which may reflect regional innovation ecosystems or Blueprinty’s sales efforts.\n\nThese systematic differences in age and region reinforce the need to control for potential confounders when estimating the effect of Blueprinty usage on patenting outcomes.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\n\n\nLet \\(Y_i\\) be the number of patents awarded to firm \\(i\\), where \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\). The probability mass function of the Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across \\(n\\) firms, the likelihood function for the full sample is the product of individual probabilities:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nTaking the natural logarithm of the likelihood gives us the log-likelihood function:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda_i + Y_i \\log \\lambda_i - \\log Y_i! \\right)\n\\]\nThis log-likelihood is what we maximize when estimating a Poisson regression model.\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_log_likelihood(lmbda, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model.\n\n    Parameters:\n    - lmbda: array-like of Poisson rates (λ_i)\n    - Y: array-like of observed counts (Y_i)\n\n    Returns:\n    - total log-likelihood (float)\n    \"\"\"\n    lmbda = np.asarray(lmbda)\n    Y = np.asarray(Y)\n\n    \n    log_lik = -lmbda + Y * np.log(lmbda) - gammaln(Y + 1)\n    return np.sum(log_lik)\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nY = df[\"patents\"].dropna().astype(float).values\n\n\nlambda_vals = np.linspace(0.1, 10, 100)\n\n\nlog_liks = [poisson_log_likelihood(np.full_like(Y, lam), Y) for lam in lambda_vals]\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks, label=\"Log-Likelihood\", color='blue')\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe plot above shows how the Poisson log-likelihood varies as we change the constant value of (), using the observed patent counts as the outcome.\nWe observe that:\n\nThe log-likelihood increases initially with (), reaches a maximum around (), and then begins to decline.\nThis peak represents the maximum likelihood estimate (MLE) for () when assuming a constant rate across all firms.\nThe shape of the curve confirms the expected behavior of the Poisson likelihood — it’s unimodal, with a clear optimal point that balances the trade-off between under- and over-predicting patent counts.\n\nThis exercise illustrates the core principle of MLE: choosing the parameter value that makes the observed data most likely under the assumed statistical model.\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\n\n\n\nTo find the maximum likelihood estimate (MLE) of (), we begin with the log-likelihood function for a Poisson model:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nTo find the MLE, we take the derivative with respect to () and set it equal to zero:\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\]\nSimplifying:\n\\[\n\\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0\n\\quad \\Rightarrow \\quad\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ():\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = n\n\\quad \\Rightarrow \\quad\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThus, the MLE of () is simply the sample mean ({Y}), which aligns with our intuition since the Poisson distribution has mean equal to ().\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\nY = df[\"patents\"].dropna().astype(float).values\n\n\ndef neg_log_likelihood(lam_array):\n    lam = lam_array[0]\n    if lam &lt;= 0:\n        return np.inf  \n    return -poisson_log_likelihood(np.full_like(Y, lam), Y)\n\n\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(1e-6, None)])\n\n\nlambda_mle = result.x[0]\nlambda_mle\n\nnp.float64(3.6846662953477973)\n\n\n\n\n\nUsing numerical optimization via scipy.optimize.minimize(), we estimated the Poisson rate parameter () that maximizes the log-likelihood of the observed data.\nThe optimizer returned:\n[ _{} ]\nThis estimate matches the sample mean of the observed patent counts, confirming the analytical result derived earlier — that the MLE for () in a Poisson distribution is the average of the data:\n[ _{} = {Y} ]\nThis result strengthens our understanding of maximum likelihood estimation and provides a solid baseline before incorporating covariates in a full regression model.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nfrom scipy.special import gammaln\nimport numpy as np\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    \"\"\"\n    Computes the log-likelihood for a Poisson regression model\n    where lambda_i = exp(X_i' * beta).\n    \n    Parameters:\n    - beta: Coefficient vector (length p)\n    - Y: Observed counts (length n)\n    - X: Covariate matrix (n x p)\n\n    Returns:\n    - Total log-likelihood (scalar)\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    X = np.asarray(X, dtype=float)\n\n    lambda_i = np.exp(X @ beta)\n    log_lik = -lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1)\n    return np.sum(log_lik)\n\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport numdifftools as nd\n\n\ndf = pd.read_csv(\"blueprinty.csv\").dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"])\n\n\ndf[\"age_centered\"] = df[\"age\"] - df[\"age\"].mean()\ndf[\"age_sq\"] = df[\"age_centered\"] ** 2\n\n\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_centered\", \"age_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.astype(float).values\nY = df[\"patents\"].astype(float).values\n\n\ndef poisson_regression_log_likelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    lambda_i = np.exp(X @ beta)\n    return np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n\n\ndef neg_log_likelihood_beta(beta):\n    return -poisson_regression_log_likelihood(beta, Y, X_matrix)\n\n\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood_beta, init_beta, method=\"BFGS\")\n\n\nbeta_mle = result.x\n\n\nhessian_fun = nd.Hessian(neg_log_likelihood_beta)\nhessian_matrix = hessian_fun(beta_mle)\ncov_matrix = np.linalg.inv(hessian_matrix)\nse_beta = np.sqrt(np.diag(cov_matrix))\n\n\nresults_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": se_beta\n})\n\nresults_table.round(4)\n\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: divide by zero encountered in matmul\n  lambda_i = np.exp(X @ beta)\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: overflow encountered in matmul\n  lambda_i = np.exp(X @ beta)\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: invalid value encountered in matmul\n  lambda_i = np.exp(X @ beta)\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: overflow encountered in matmul\n  lambda_i = np.exp(X @ beta)\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_79449/542943536.py:29: RuntimeWarning: overflow encountered in exp\n  lambda_i = np.exp(X @ beta)\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.3447\n0.0383\n\n\n1\nage_centered\n-0.0080\n0.0021\n\n\n2\nage_sq\n-0.0030\n0.0003\n\n\n3\niscustomer\n0.2076\n0.0309\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n\n\n6\nregion_South\n0.0566\n0.0527\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n\nX_sm = sm.add_constant(X.drop(columns=\"intercept\")).astype(float)\nY_sm = Y.astype(float)\n\n\nmodel = sm.GLM(Y_sm, X_sm, family=sm.families.Poisson())\nresults = model.fit()\n\n\nsm_results_table = pd.DataFrame({\n    \"Variable\": results.params.index,\n    \"Coefficient (sm.GLM)\": results.params.values,\n    \"Std. Error (sm.GLM)\": results.bse.values\n})\n\nsm_results_table.round(4)\n\n\n\n\n\n\n\n\nVariable\nCoefficient (sm.GLM)\nStd. Error (sm.GLM)\n\n\n\n\n0\nconst\n1.3447\n0.0384\n\n\n1\nage_centered\n-0.0080\n0.0021\n\n\n2\nage_sq\n-0.0030\n0.0003\n\n\n3\niscustomer\n0.2076\n0.0309\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n\n\n6\nregion_South\n0.0566\n0.0527\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n\n\n\n\n\n\n\n\n\n\nThe Poisson regression estimates the relationship between firm characteristics and the number of patents awarded. Key interpretations include:\n\nIntercept (const = 1.3447): This is the expected log count of patents for a non-customer firm with average age and located in the baseline region (the region omitted from dummy encoding). Exponentiating gives an expected count of ((1.3447) ) patents.\nCustomer Status (iscustomer = 0.2076, SE = 0.0309): This coefficient is statistically significant and positive, indicating that Blueprinty customers are associated with a 23.1% higher expected number of patents compared to non-customers, holding all else constant.\n[ (0.2076) ]\nFirm Age Effects:\n\nThe negative coefficient on age_centered (-0.0080) suggests that, near the average, older firms have a slightly lower expected number of patents.\nThe negative coefficient on age_sq (-0.0030) implies diminishing returns or a concave relationship: patenting productivity decreases faster at higher ages.\n\nRegional Effects: The region coefficients are relatively small, and most are not statistically distinguishable from zero given their standard errors. This suggests that regional location has limited independent effect on patenting after accounting for other covariates.\n\nOverall, the model supports the conclusion that Blueprinty customers tend to be more successful in securing patents, even after controlling for firm age and region.\n\nX_0_mat = X_sm.copy()\nX_1_mat = X_sm.copy()\n\n\nX_0_mat[\"iscustomer\"] = 0\nX_1_mat[\"iscustomer\"] = 1\n\n\ny_pred_0 = results.predict(X_0_mat)\ny_pred_1 = results.predict(X_1_mat)\n\n\ndelta_y = y_pred_1 - y_pred_0\navg_treatment_effect = np.mean(delta_y)\n\navg_treatment_effect\n\nnp.float64(0.7927680710453266)\n\n\n\n\n\nTo estimate the real-world impact of Blueprinty’s software on patenting success, we conducted a counterfactual analysis. Specifically, we predicted the number of patents each firm would file under two hypothetical scenarios:\n\nScenario 1 (X_0): All firms are non-customers (iscustomer = 0)\nScenario 2 (X_1): All firms are customers (iscustomer = 1)\n\nWe then computed the difference in predicted patents for each firm and averaged those differences. The result:\n[ = {Y}{} - {Y}{} = 0.793 ]\nThis means that, on average, Blueprinty customers are expected to file approximately 0.79 more patents than they would have if they weren’t customers, holding age and region constant.\nThis provides strong evidence that using Blueprinty’s software is associated with a meaningful improvement in patenting outcomes."
  },
  {
    "objectID": "projects/project1/hw2_questions.html#airbnb-case-study",
    "href": "projects/project1/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n\ndf = pd.read_csv(\"airbnb.csv\")\n\ncols = [\n    \"number_of_reviews\", \"days\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"room_type\", \"instant_bookable\"\n]\ndf = df[cols].dropna()\n\ndf[\"instant_bookable\"] = df[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\nroom_dummies = pd.get_dummies(df[\"room_type\"], prefix=\"room\", drop_first=True)\n\nX = pd.concat([\n    df[[\"days\", \"price\", \"review_scores_cleanliness\", \n        \"review_scores_location\", \"review_scores_value\", \n        \"instant_bookable\"]],\n    room_dummies\n], axis=1)\nX = sm.add_constant(X).astype(float)\n\nY = df[\"number_of_reviews\"].astype(float)\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresults = model.fit()\n\n\nresults.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30346\n\n\nModel:\nGLM\nDf Residuals:\n30337\n\n\nModel Family:\nPoisson\nDf Model:\n8\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.3016e+05\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n9.3800e+05\n\n\nTime:\n15:17:19\nPearson chi2:\n1.39e+06\n\n\nNo. Iterations:\n10\nPseudo R-squ. (CS):\n0.6658\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.5005\n0.015\n227.783\n0.000\n3.470\n3.531\n\n\ndays\n5.056e-05\n3.86e-07\n130.943\n0.000\n4.98e-05\n5.13e-05\n\n\nprice\n-8.284e-06\n7.55e-06\n-1.097\n0.272\n-2.31e-05\n6.51e-06\n\n\nreview_scores_cleanliness\n0.1128\n0.001\n75.860\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0823\n0.002\n-51.866\n0.000\n-0.085\n-0.079\n\n\nreview_scores_value\n-0.0895\n0.002\n-49.909\n0.000\n-0.093\n-0.086\n\n\ninstant_bookable\n0.3446\n0.003\n119.714\n0.000\n0.339\n0.350\n\n\nroom_Private room\n-0.0215\n0.003\n-8.025\n0.000\n-0.027\n-0.016\n\n\nroom_Shared room\n-0.2586\n0.009\n-30.198\n0.000\n-0.275\n-0.242\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nWe use the number of reviews as a proxy for the number of bookings. A Poisson regression model was fit to explain variation in review counts based on listing characteristics.\nKey takeaways:\n\ndays (coef = 5.06e-05): Listings that have been active longer receive more reviews, as expected. The effect is small but statistically significant.\nprice (coef = -8.28e-06, p = 0.272): Price does not have a statistically significant effect on the number of reviews after controlling for other factors.\nreview_scores_cleanliness (coef = 0.1128): Cleanliness ratings are positively associated with reviews. A one-point increase in cleanliness score is associated with a ~11.9% increase in expected reviews, all else equal ((e^{0.1128} )).\nreview_scores_location (coef = -0.0823) and review_scores_value (coef = -0.0895): Surprisingly, higher scores in these categories are negatively associated with review counts. This could reflect less variation or more passive feedback in these dimensions.\ninstant_bookable (coef = 0.3446): Listings that are instantly bookable receive significantly more reviews — approximately 41% more than non-instant-bookable ones ((e^{0.3446} )).\nroom_type:\n\nPrivate room (coef = -0.0215): Slightly fewer reviews than entire homes, though effect is small.\nShared room (coef = -0.2586): Substantially fewer reviews — about 23% fewer than entire homes ((e^{-0.2586} )).\n\n\n\nOverall, the model shows that cleanliness, availability (instant booking), and room type are strong drivers of Airbnb engagement as measured through reviews."
  },
  {
    "objectID": "projects/project 3/hw3_questions.html",
    "href": "projects/project 3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/project 3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/project 3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/project 3/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/project 3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define product features\nbrands = [\"Netflix\", \"Prime\", \"Hulu\"]\nads = [\"Yes\", \"No\"]\nprices = np.arange(8, 33, 4)  # $8 to $32 by $4\n\n# Create all possible combinations of features\nproduct_profiles = pd.DataFrame([\n    (b, a, p) for b in brands for a in ads for p in prices\n], columns=[\"Brand\", \"Ads\", \"Price\"])\n\n# Define true part-worth utilities\nbrand_utils = {\"Netflix\": 1.0, \"Prime\": 0.5, \"Hulu\": 0.0}\nads_utils = {\"Yes\": -0.8, \"No\": 0.0}\nprice_util = lambda p: -0.1 * p\n\n# Function to simulate choices for one respondent\ndef simulate_responder(id, n_tasks=10, n_alts=3):\n    responses = []\n    for task in range(n_tasks):\n        choice_set = product_profiles.sample(n=n_alts).copy()\n        choice_set[\"Resp\"] = id\n        choice_set[\"Task\"] = task + 1\n        choice_set[\"Utility\"] = (\n            choice_set[\"Brand\"].map(brand_utils) +\n            choice_set[\"Ads\"].map(ads_utils) +\n            choice_set[\"Price\"].apply(price_util)\n        )\n        # Add Gumbel noise\n        choice_set[\"Error\"] = -np.log(-np.log(np.random.rand(n_alts)))\n        choice_set[\"U_total\"] = choice_set[\"Utility\"] + choice_set[\"Error\"]\n        choice_set[\"Chosen\"] = (choice_set[\"U_total\"] == choice_set[\"U_total\"].max()).astype(int)\n        responses.append(choice_set)\n    return pd.concat(responses)\n\n# Simulate data for all respondents\ndata = pd.concat([simulate_responder(i) for i in range(1, 101)], ignore_index=True)\n\n# Keep only observable columns\ndf = data[[\"Resp\", \"Task\", \"Brand\", \"Ads\", \"Price\", \"Chosen\"]]\ndf.head()\n\n\n\n\n\n\n\n\nResp\nTask\nBrand\nAds\nPrice\nChosen\n\n\n\n\n0\n1\n1\nPrime\nNo\n32\n0\n\n\n1\n1\n1\nNetflix\nNo\n28\n0\n\n\n2\n1\n1\nNetflix\nNo\n24\n1\n\n\n3\n1\n2\nHulu\nNo\n28\n0\n\n\n4\n1\n2\nHulu\nNo\n8\n1\n\n\n\n\n\n\n\n\n3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n# One-hot encode features\ndf_prepared = pd.get_dummies(df, columns=[\"Brand\", \"Ads\"], drop_first=True)\n\n# Create model matrix\nX_cols = ['Brand_Netflix', 'Brand_Prime', 'Ads_Yes', 'Price']\nX = df_prepared[X_cols].astype(np.float64).values\ny = df_prepared[\"Chosen\"].astype(int).values\ngroup_ids = df_prepared[\"Resp\"].astype(int).astype(\"category\").cat.codes * 10 + df_prepared[\"Task\"] - 1\ngroup = group_ids.values  # unique task IDs\n\n\n\n4. Estimation via Maximum Likelihood\n\nfrom scipy.special import logsumexp\n\n# Log-likelihood using grouped softmax\ndef neg_log_likelihood(beta):\n    utilities = X @ beta\n    log_like = 0\n    for g in np.unique(group):\n        idx = group == g\n        u = utilities[idx]\n        y_group = y[idx]\n        log_probs = u - logsumexp(u)\n        log_like += np.sum(y_group * log_probs)\n    return -log_like  # negative for minimization\n\n\nfrom scipy.optimize import minimize\n\n# Initial guess\nbeta_init = np.zeros(X.shape[1])\n\n# Estimate MLE\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\nbeta_hat = result.x\nvcov = result.hess_inv\nse = np.sqrt(np.diag(vcov))\nz = 1.96\n\n# 95% Confidence intervals\nci_bounds = np.vstack((beta_hat - z * se, beta_hat + z * se)).T\n\n# Final results table\nparam_labels = ['beta_Netflix', 'beta_Prime', 'beta_Ads', 'beta_Price']\nresults = pd.DataFrame({\n    \"Parameter\": param_labels,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se,\n    \"CI Lower\": ci_bounds[:, 0],\n    \"CI Upper\": ci_bounds[:, 1]\n})\nresults\n\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/3336442945.py:5: RuntimeWarning: divide by zero encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/3336442945.py:5: RuntimeWarning: overflow encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/3336442945.py:5: RuntimeWarning: invalid value encountered in matmul\n  utilities = X @ beta\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\nCI Lower\nCI Upper\n\n\n\n\n0\nbeta_Netflix\n0.665459\n0.018884\n0.628447\n0.702472\n\n\n1\nbeta_Prime\n0.328906\n0.025258\n0.279401\n0.378410\n\n\n2\nbeta_Ads\n-0.474876\n0.033683\n-0.540896\n-0.408857\n\n\n3\nbeta_Price\n-0.063683\n0.003821\n-0.071173\n-0.056193\n\n\n\n\n\n\n\nThe estimated parameters from the multinomial logit model represent the influence of each product attribute on a consumer’s likelihood of choosing a particular option.\nAs shown in the table above:\n\nPositive coefficients (e.g., for Brand_P and Brand_H) indicate that those attributes increase the utility of the product, making it more likely to be chosen.\nNegative coefficients (e.g., for Ad_Yes and Price) suggest those features reduce product attractiveness and decrease the probability of choice.\n\nThe price coefficient being negative aligns with economic theory — higher prices discourage selection. The negative ad coefficient reflects a common preference for ad-free content. In contrast, positive brand coefficients imply that consumers value more familiar or premium brands over the baseline option.\nThese results offer interpretable, statistically significant insights into how each attribute contributes to product choice, and they serve as a baseline for comparison with the Bayesian estimates in the next section.\n\n\n5. Estimation via Bayesian Methods\n\nimport numpy as np\n\n# Prior: N(0, 5) for binary vars, N(0,1) for price\nprior_sd = np.array([5.0, 5.0, 5.0, 1.0])  # [brand_P, brand_H, ad_Yes, price]\n\ndef log_prior(beta):\n    return -0.5 * np.sum((beta / prior_sd)**2)\n\ndef log_likelihood(beta):\n    utilities = X @ beta\n    log_like = 0\n    for g in np.unique(group):\n        idx = group == g\n        u = utilities[idx]\n        y_g = y[idx]\n        log_probs = u - logsumexp(u)\n        log_like += np.sum(y_g * log_probs)\n    return log_like\n\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings Sampler\nn_iter = 11000\nburn_in = 1000\nbeta_samples = np.zeros((n_iter, X.shape[1]))\ncurrent = np.zeros(X.shape[1])\n\nfor i in range(n_iter):\n    proposal = current + np.random.normal(0, 0.1, size=current.shape)\n    log_accept_ratio = log_posterior(proposal) - log_posterior(current)\n    if np.log(np.random.rand()) &lt; log_accept_ratio:\n        current = proposal\n    beta_samples[i] = current\n\n# Remove burn-in\nbeta_post = beta_samples[burn_in:]\n\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/51646031.py:10: RuntimeWarning: divide by zero encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/51646031.py:10: RuntimeWarning: overflow encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/51646031.py:10: RuntimeWarning: invalid value encountered in matmul\n  utilities = X @ beta\n\n\n\nimport matplotlib.pyplot as plt\n\n# Select beta_Price (last column, index = 3)\nprice_samples = beta_post[:, 3]\n\n# Plot: Trace plot\nplt.figure(figsize=(12, 4))\nplt.plot(price_samples, lw=0.5)\nplt.title(\"Trace Plot of beta_Price\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"beta_Price\")\nplt.grid(True)\nplt.show()\n\n# Plot: Histogram of posterior distribution\nplt.figure(figsize=(8, 4))\nplt.hist(price_samples, bins=50, density=True, alpha=0.7)\nplt.title(\"Posterior Distribution of beta_Price\")\nplt.xlabel(\"beta_Price\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compute posterior summaries\nposterior_means = beta_post.mean(axis=0)\nposterior_sds = beta_post.std(axis=0)\ncred_int_95 = np.percentile(beta_post, [2.5, 97.5], axis=0).T\n\n# Create results table\nparams = ['beta_brand_P', 'beta_brand_H', 'beta_ad_Yes', 'beta_price']\nbayes_df = pd.DataFrame({\n    \"Parameter\": params,\n    \"Posterior Mean\": posterior_means,\n    \"Posterior Std. Dev.\": posterior_sds,\n    \"95% Credible Lower\": cred_int_95[:, 0],\n    \"95% Credible Upper\": cred_int_95[:, 1]\n})\n\nbayes_df\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior Std. Dev.\n95% Credible Lower\n95% Credible Upper\n\n\n\n\n0\nbeta_brand_P\n0.653731\n0.078918\n0.513701\n0.817531\n\n\n1\nbeta_brand_H\n0.314416\n0.090643\n0.138598\n0.472656\n\n\n2\nbeta_ad_Yes\n-0.475267\n0.069819\n-0.610270\n-0.336048\n\n\n3\nbeta_price\n-0.063525\n0.004603\n-0.072898\n-0.054416\n\n\n\n\n\n\n\n\n\nInterpretation of MLE Results\nThe table of maximum likelihood estimates (MLEs) provides insight into how different product attributes influence consumer choice.\n\nBrand_P and Brand_H both have positive coefficients, indicating that these brands are more preferred compared to the reference brand (likely Hulu, since it’s not explicitly shown as a variable). Among the two, the higher value for Brand_P suggests it is the most favored.\nAd_Yes has a negative coefficient, confirming that the presence of advertisements reduces the likelihood of a product being selected.\nPrice has a negative coefficient, as expected, showing that higher prices decrease the utility of the product and reduce the probability of choice.\n\nThe standard errors are relatively small, and all 95% confidence intervals exclude zero, suggesting the estimates are statistically significant. The signs and relative magnitudes of the coefficients align with consumer intuition: people prefer ad-free, affordable options from more popular brands.\nIn summary, the MNL model confirms expected consumer preferences and yields interpretable results that help quantify how much each feature matters in driving choices.\n\n\nDiscussion\nIf we had not simulated the data ourselves, we would interpret the estimated parameters as insights drawn from real consumer preferences. In this case, the model suggests that Netflix is the most preferred brand among the three options, followed by Prime Video, with Hulu serving as the baseline (i.e., the least preferred).\nThe fact that \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) tells us that, all else equal, consumers derive more utility from Netflix than from Prime. This could reflect stronger brand loyalty, better content perception, or a more favorable user experience associated with Netflix.\nThe negative sign on \\(\\beta_\\text{price}\\) aligns well with consumer economic behavior — as price increases, the likelihood of choosing a product decreases. This is consistent with standard theory: price acts as a deterrent when consumers are making trade-offs between otherwise similar options.\nOverall, the direction and magnitude of the coefficients not only make sense but also reflect patterns we would expect to see in real-world streaming service choice behavior. The model successfully captures key drivers of utility and provides interpretable, data-driven insights.\nTo simulate data from a hierarchical (multi-level) logit model, we would need to allow each respondent to have their own set of preference parameters (β). Instead of assigning a single global β vector to everyone, we would draw each individual’s β from a population-level distribution — typically a multivariate normal distribution with its own mean vector and covariance matrix.\nThis results in what’s called a random-parameters logit model, where individual heterogeneity is explicitly modeled. In simulation, for each respondent \\(i\\), we would generate:\n[ _i (, ) ]\nAnd use \\(\\beta_i\\) to compute utilities and simulate choices for that individual.\nTo estimate such a model, we would need to use a Bayesian method like Metropolis-within-Gibbs, Hamiltonian Monte Carlo, or variational inference, since the model includes latent parameters at two levels: - The individual-level betas (\\(\\beta_i\\)) - The population-level hyperparameters (\\(\\mu, \\Sigma\\))\nSuch models are much more flexible and better suited to real-world conjoint data, where preferences naturally vary across individuals. This structure captures both the overall market trend and personalized utility drivers at the respondent level."
  },
  {
    "objectID": "projects/project 3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/project 3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n# One-hot encode features\ndf_prepared = pd.get_dummies(df, columns=[\"Brand\", \"Ads\"], drop_first=True)\n\n# Create model matrix\nX_cols = ['Brand_Netflix', 'Brand_Prime', 'Ads_Yes', 'Price']\nX = df_prepared[X_cols].astype(np.float64).values\ny = df_prepared[\"Chosen\"].astype(int).values\ngroup_ids = df_prepared[\"Resp\"].astype(int).astype(\"category\").cat.codes * 10 + df_prepared[\"Task\"] - 1\ngroup = group_ids.values  # unique task IDs"
  },
  {
    "objectID": "projects/project 3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/project 3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nfrom scipy.special import logsumexp\n\n# Log-likelihood using grouped softmax\ndef neg_log_likelihood(beta):\n    utilities = X @ beta\n    log_like = 0\n    for g in np.unique(group):\n        idx = group == g\n        u = utilities[idx]\n        y_group = y[idx]\n        log_probs = u - logsumexp(u)\n        log_like += np.sum(y_group * log_probs)\n    return -log_like  # negative for minimization\n\n\nfrom scipy.optimize import minimize\n\n# Initial guess\nbeta_init = np.zeros(X.shape[1])\n\n# Estimate MLE\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\nbeta_hat = result.x\nvcov = result.hess_inv\nse = np.sqrt(np.diag(vcov))\nz = 1.96\n\n# 95% Confidence intervals\nci_bounds = np.vstack((beta_hat - z * se, beta_hat + z * se)).T\n\n# Final results table\nparam_labels = ['beta_Netflix', 'beta_Prime', 'beta_Ads', 'beta_Price']\nresults = pd.DataFrame({\n    \"Parameter\": param_labels,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se,\n    \"CI Lower\": ci_bounds[:, 0],\n    \"CI Upper\": ci_bounds[:, 1]\n})\nresults\n\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/3336442945.py:5: RuntimeWarning: divide by zero encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/3336442945.py:5: RuntimeWarning: overflow encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/3336442945.py:5: RuntimeWarning: invalid value encountered in matmul\n  utilities = X @ beta\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\nCI Lower\nCI Upper\n\n\n\n\n0\nbeta_Netflix\n0.665459\n0.018884\n0.628447\n0.702472\n\n\n1\nbeta_Prime\n0.328906\n0.025258\n0.279401\n0.378410\n\n\n2\nbeta_Ads\n-0.474876\n0.033683\n-0.540896\n-0.408857\n\n\n3\nbeta_Price\n-0.063683\n0.003821\n-0.071173\n-0.056193\n\n\n\n\n\n\n\nThe estimated parameters from the multinomial logit model represent the influence of each product attribute on a consumer’s likelihood of choosing a particular option.\nAs shown in the table above:\n\nPositive coefficients (e.g., for Brand_P and Brand_H) indicate that those attributes increase the utility of the product, making it more likely to be chosen.\nNegative coefficients (e.g., for Ad_Yes and Price) suggest those features reduce product attractiveness and decrease the probability of choice.\n\nThe price coefficient being negative aligns with economic theory — higher prices discourage selection. The negative ad coefficient reflects a common preference for ad-free content. In contrast, positive brand coefficients imply that consumers value more familiar or premium brands over the baseline option.\nThese results offer interpretable, statistically significant insights into how each attribute contributes to product choice, and they serve as a baseline for comparison with the Bayesian estimates in the next section."
  },
  {
    "objectID": "projects/project 3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/project 3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nimport numpy as np\n\n# Prior: N(0, 5) for binary vars, N(0,1) for price\nprior_sd = np.array([5.0, 5.0, 5.0, 1.0])  # [brand_P, brand_H, ad_Yes, price]\n\ndef log_prior(beta):\n    return -0.5 * np.sum((beta / prior_sd)**2)\n\ndef log_likelihood(beta):\n    utilities = X @ beta\n    log_like = 0\n    for g in np.unique(group):\n        idx = group == g\n        u = utilities[idx]\n        y_g = y[idx]\n        log_probs = u - logsumexp(u)\n        log_like += np.sum(y_g * log_probs)\n    return log_like\n\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings Sampler\nn_iter = 11000\nburn_in = 1000\nbeta_samples = np.zeros((n_iter, X.shape[1]))\ncurrent = np.zeros(X.shape[1])\n\nfor i in range(n_iter):\n    proposal = current + np.random.normal(0, 0.1, size=current.shape)\n    log_accept_ratio = log_posterior(proposal) - log_posterior(current)\n    if np.log(np.random.rand()) &lt; log_accept_ratio:\n        current = proposal\n    beta_samples[i] = current\n\n# Remove burn-in\nbeta_post = beta_samples[burn_in:]\n\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/51646031.py:10: RuntimeWarning: divide by zero encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/51646031.py:10: RuntimeWarning: overflow encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_93782/51646031.py:10: RuntimeWarning: invalid value encountered in matmul\n  utilities = X @ beta\n\n\n\nimport matplotlib.pyplot as plt\n\n# Select beta_Price (last column, index = 3)\nprice_samples = beta_post[:, 3]\n\n# Plot: Trace plot\nplt.figure(figsize=(12, 4))\nplt.plot(price_samples, lw=0.5)\nplt.title(\"Trace Plot of beta_Price\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"beta_Price\")\nplt.grid(True)\nplt.show()\n\n# Plot: Histogram of posterior distribution\nplt.figure(figsize=(8, 4))\nplt.hist(price_samples, bins=50, density=True, alpha=0.7)\nplt.title(\"Posterior Distribution of beta_Price\")\nplt.xlabel(\"beta_Price\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compute posterior summaries\nposterior_means = beta_post.mean(axis=0)\nposterior_sds = beta_post.std(axis=0)\ncred_int_95 = np.percentile(beta_post, [2.5, 97.5], axis=0).T\n\n# Create results table\nparams = ['beta_brand_P', 'beta_brand_H', 'beta_ad_Yes', 'beta_price']\nbayes_df = pd.DataFrame({\n    \"Parameter\": params,\n    \"Posterior Mean\": posterior_means,\n    \"Posterior Std. Dev.\": posterior_sds,\n    \"95% Credible Lower\": cred_int_95[:, 0],\n    \"95% Credible Upper\": cred_int_95[:, 1]\n})\n\nbayes_df\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior Std. Dev.\n95% Credible Lower\n95% Credible Upper\n\n\n\n\n0\nbeta_brand_P\n0.653731\n0.078918\n0.513701\n0.817531\n\n\n1\nbeta_brand_H\n0.314416\n0.090643\n0.138598\n0.472656\n\n\n2\nbeta_ad_Yes\n-0.475267\n0.069819\n-0.610270\n-0.336048\n\n\n3\nbeta_price\n-0.063525\n0.004603\n-0.072898\n-0.054416"
  },
  {
    "objectID": "projects/project 3/hw3_questions.html#interpretation-of-mle-results",
    "href": "projects/project 3/hw3_questions.html#interpretation-of-mle-results",
    "title": "Multinomial Logit Model",
    "section": "Interpretation of MLE Results",
    "text": "Interpretation of MLE Results\nThe table of maximum likelihood estimates (MLEs) provides insight into how different product attributes influence consumer choice.\n\nBrand_P and Brand_H both have positive coefficients, indicating that these brands are more preferred compared to the reference brand (likely Hulu, since it’s not explicitly shown as a variable). Among the two, the higher value for Brand_P suggests it is the most favored.\nAd_Yes has a negative coefficient, confirming that the presence of advertisements reduces the likelihood of a product being selected.\nPrice has a negative coefficient, as expected, showing that higher prices decrease the utility of the product and reduce the probability of choice.\n\nThe standard errors are relatively small, and all 95% confidence intervals exclude zero, suggesting the estimates are statistically significant. The signs and relative magnitudes of the coefficients align with consumer intuition: people prefer ad-free, affordable options from more popular brands.\nIn summary, the MNL model confirms expected consumer preferences and yields interpretable results that help quantify how much each feature matters in driving choices."
  },
  {
    "objectID": "projects/project3/hw3_questions.html",
    "href": "projects/project3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/project3/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/project3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define product features\nbrands = [\"Netflix\", \"Prime\", \"Hulu\"]\nads = [\"Yes\", \"No\"]\nprices = np.arange(8, 33, 4)  # $8 to $32 by $4\n\n# Create all possible combinations of features\nproduct_profiles = pd.DataFrame([\n    (b, a, p) for b in brands for a in ads for p in prices\n], columns=[\"Brand\", \"Ads\", \"Price\"])\n\n# Define true part-worth utilities\nbrand_utils = {\"Netflix\": 1.0, \"Prime\": 0.5, \"Hulu\": 0.0}\nads_utils = {\"Yes\": -0.8, \"No\": 0.0}\nprice_util = lambda p: -0.1 * p\n\n# Function to simulate choices for one respondent\ndef simulate_responder(id, n_tasks=10, n_alts=3):\n    responses = []\n    for task in range(n_tasks):\n        choice_set = product_profiles.sample(n=n_alts).copy()\n        choice_set[\"Resp\"] = id\n        choice_set[\"Task\"] = task + 1\n        choice_set[\"Utility\"] = (\n            choice_set[\"Brand\"].map(brand_utils) +\n            choice_set[\"Ads\"].map(ads_utils) +\n            choice_set[\"Price\"].apply(price_util)\n        )\n        # Add Gumbel noise\n        choice_set[\"Error\"] = -np.log(-np.log(np.random.rand(n_alts)))\n        choice_set[\"U_total\"] = choice_set[\"Utility\"] + choice_set[\"Error\"]\n        choice_set[\"Chosen\"] = (choice_set[\"U_total\"] == choice_set[\"U_total\"].max()).astype(int)\n        responses.append(choice_set)\n    return pd.concat(responses)\n\n# Simulate data for all respondents\ndata = pd.concat([simulate_responder(i) for i in range(1, 101)], ignore_index=True)\n\n# Keep only observable columns\ndf = data[[\"Resp\", \"Task\", \"Brand\", \"Ads\", \"Price\", \"Chosen\"]]\ndf.head()\n\n\n\n\n\n\n\n\nResp\nTask\nBrand\nAds\nPrice\nChosen\n\n\n\n\n0\n1\n1\nPrime\nNo\n32\n0\n\n\n1\n1\n1\nNetflix\nNo\n28\n0\n\n\n2\n1\n1\nNetflix\nNo\n24\n1\n\n\n3\n1\n2\nHulu\nNo\n28\n0\n\n\n4\n1\n2\nHulu\nNo\n8\n1"
  },
  {
    "objectID": "projects/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n# One-hot encode features\ndf_prepared = pd.get_dummies(df, columns=[\"Brand\", \"Ads\"], drop_first=True)\n\n# Create model matrix\nX_cols = ['Brand_Netflix', 'Brand_Prime', 'Ads_Yes', 'Price']\nX = df_prepared[X_cols].astype(np.float64).values\ny = df_prepared[\"Chosen\"].astype(int).values\ngroup_ids = df_prepared[\"Resp\"].astype(int).astype(\"category\").cat.codes * 10 + df_prepared[\"Task\"] - 1\ngroup = group_ids.values  # unique task IDs"
  },
  {
    "objectID": "projects/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nfrom scipy.special import logsumexp\n\n# Log-likelihood using grouped softmax\ndef neg_log_likelihood(beta):\n    utilities = X @ beta\n    log_like = 0\n    for g in np.unique(group):\n        idx = group == g\n        u = utilities[idx]\n        y_group = y[idx]\n        log_probs = u - logsumexp(u)\n        log_like += np.sum(y_group * log_probs)\n    return -log_like  # negative for minimization\n\n\nfrom scipy.optimize import minimize\n\n# Initial guess\nbeta_init = np.zeros(X.shape[1])\n\n# Estimate MLE\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\nbeta_hat = result.x\nvcov = result.hess_inv\nse = np.sqrt(np.diag(vcov))\nz = 1.96\n\n# 95% Confidence intervals\nci_bounds = np.vstack((beta_hat - z * se, beta_hat + z * se)).T\n\n# Final results table\nparam_labels = ['beta_Netflix', 'beta_Prime', 'beta_Ads', 'beta_Price']\nresults = pd.DataFrame({\n    \"Parameter\": param_labels,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se,\n    \"CI Lower\": ci_bounds[:, 0],\n    \"CI Upper\": ci_bounds[:, 1]\n})\nresults\n\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_94578/3336442945.py:5: RuntimeWarning: divide by zero encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_94578/3336442945.py:5: RuntimeWarning: overflow encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_94578/3336442945.py:5: RuntimeWarning: invalid value encountered in matmul\n  utilities = X @ beta\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\nCI Lower\nCI Upper\n\n\n\n\n0\nbeta_Netflix\n0.665459\n0.018884\n0.628447\n0.702472\n\n\n1\nbeta_Prime\n0.328906\n0.025258\n0.279401\n0.378410\n\n\n2\nbeta_Ads\n-0.474876\n0.033683\n-0.540896\n-0.408857\n\n\n3\nbeta_Price\n-0.063683\n0.003821\n-0.071173\n-0.056193\n\n\n\n\n\n\n\nThe estimated parameters from the multinomial logit model represent the influence of each product attribute on a consumer’s likelihood of choosing a particular option.\nAs shown in the table above:\n\nPositive coefficients (e.g., for Brand_P and Brand_H) indicate that those attributes increase the utility of the product, making it more likely to be chosen.\nNegative coefficients (e.g., for Ad_Yes and Price) suggest those features reduce product attractiveness and decrease the probability of choice.\n\nThe price coefficient being negative aligns with economic theory — higher prices discourage selection. The negative ad coefficient reflects a common preference for ad-free content. In contrast, positive brand coefficients imply that consumers value more familiar or premium brands over the baseline option.\nThese results offer interpretable, statistically significant insights into how each attribute contributes to product choice, and they serve as a baseline for comparison with the Bayesian estimates in the next section."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nimport numpy as np\n\n# Prior: N(0, 5) for binary vars, N(0,1) for price\nprior_sd = np.array([5.0, 5.0, 5.0, 1.0])  # [brand_P, brand_H, ad_Yes, price]\n\ndef log_prior(beta):\n    return -0.5 * np.sum((beta / prior_sd)**2)\n\ndef log_likelihood(beta):\n    utilities = X @ beta\n    log_like = 0\n    for g in np.unique(group):\n        idx = group == g\n        u = utilities[idx]\n        y_g = y[idx]\n        log_probs = u - logsumexp(u)\n        log_like += np.sum(y_g * log_probs)\n    return log_like\n\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings Sampler\nn_iter = 11000\nburn_in = 1000\nbeta_samples = np.zeros((n_iter, X.shape[1]))\ncurrent = np.zeros(X.shape[1])\n\nfor i in range(n_iter):\n    proposal = current + np.random.normal(0, 0.1, size=current.shape)\n    log_accept_ratio = log_posterior(proposal) - log_posterior(current)\n    if np.log(np.random.rand()) &lt; log_accept_ratio:\n        current = proposal\n    beta_samples[i] = current\n\n# Remove burn-in\nbeta_post = beta_samples[burn_in:]\n\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_94578/51646031.py:10: RuntimeWarning: divide by zero encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_94578/51646031.py:10: RuntimeWarning: overflow encountered in matmul\n  utilities = X @ beta\n/var/folders/sn/qsyx4jm96k7_sk9l3hfkp_b40000gn/T/ipykernel_94578/51646031.py:10: RuntimeWarning: invalid value encountered in matmul\n  utilities = X @ beta\n\n\n\nimport matplotlib.pyplot as plt\n\n# Select beta_Price (last column, index = 3)\nprice_samples = beta_post[:, 3]\n\n# Plot: Trace plot\nplt.figure(figsize=(12, 4))\nplt.plot(price_samples, lw=0.5)\nplt.title(\"Trace Plot of beta_Price\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"beta_Price\")\nplt.grid(True)\nplt.show()\n\n# Plot: Histogram of posterior distribution\nplt.figure(figsize=(8, 4))\nplt.hist(price_samples, bins=50, density=True, alpha=0.7)\nplt.title(\"Posterior Distribution of beta_Price\")\nplt.xlabel(\"beta_Price\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compute posterior summaries\nposterior_means = beta_post.mean(axis=0)\nposterior_sds = beta_post.std(axis=0)\ncred_int_95 = np.percentile(beta_post, [2.5, 97.5], axis=0).T\n\n# Create results table\nparams = ['beta_brand_P', 'beta_brand_H', 'beta_ad_Yes', 'beta_price']\nbayes_df = pd.DataFrame({\n    \"Parameter\": params,\n    \"Posterior Mean\": posterior_means,\n    \"Posterior Std. Dev.\": posterior_sds,\n    \"95% Credible Lower\": cred_int_95[:, 0],\n    \"95% Credible Upper\": cred_int_95[:, 1]\n})\n\nbayes_df\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior Std. Dev.\n95% Credible Lower\n95% Credible Upper\n\n\n\n\n0\nbeta_brand_P\n0.653731\n0.078918\n0.513701\n0.817531\n\n\n1\nbeta_brand_H\n0.314416\n0.090643\n0.138598\n0.472656\n\n\n2\nbeta_ad_Yes\n-0.475267\n0.069819\n-0.610270\n-0.336048\n\n\n3\nbeta_price\n-0.063525\n0.004603\n-0.072898\n-0.054416"
  },
  {
    "objectID": "projects/project3/hw3_questions.html#interpretation-of-mle-results",
    "href": "projects/project3/hw3_questions.html#interpretation-of-mle-results",
    "title": "Multinomial Logit Model",
    "section": "Interpretation of MLE Results",
    "text": "Interpretation of MLE Results\nThe table of maximum likelihood estimates (MLEs) provides insight into how different product attributes influence consumer choice.\n\nBrand_P and Brand_H both have positive coefficients, indicating that these brands are more preferred compared to the reference brand (likely Hulu, since it’s not explicitly shown as a variable). Among the two, the higher value for Brand_P suggests it is the most favored.\nAd_Yes has a negative coefficient, confirming that the presence of advertisements reduces the likelihood of a product being selected.\nPrice has a negative coefficient, as expected, showing that higher prices decrease the utility of the product and reduce the probability of choice.\n\nThe standard errors are relatively small, and all 95% confidence intervals exclude zero, suggesting the estimates are statistically significant. The signs and relative magnitudes of the coefficients align with consumer intuition: people prefer ad-free, affordable options from more popular brands.\nIn summary, the MNL model confirms expected consumer preferences and yields interpretable results that help quantify how much each feature matters in driving choices."
  }
]
---
title: "Multinomial Logit Model"
author: "Your Name"
date: today
---


This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}

```{python}
import pandas as pd
import numpy as np

# Set seed for reproducibility
np.random.seed(123)

# Define product features
brands = ["Netflix", "Prime", "Hulu"]
ads = ["Yes", "No"]
prices = np.arange(8, 33, 4)  # $8 to $32 by $4

# Create all possible combinations of features
product_profiles = pd.DataFrame([
    (b, a, p) for b in brands for a in ads for p in prices
], columns=["Brand", "Ads", "Price"])

# Define true part-worth utilities
brand_utils = {"Netflix": 1.0, "Prime": 0.5, "Hulu": 0.0}
ads_utils = {"Yes": -0.8, "No": 0.0}
price_util = lambda p: -0.1 * p

# Function to simulate choices for one respondent
def simulate_responder(id, n_tasks=10, n_alts=3):
    responses = []
    for task in range(n_tasks):
        choice_set = product_profiles.sample(n=n_alts).copy()
        choice_set["Resp"] = id
        choice_set["Task"] = task + 1
        choice_set["Utility"] = (
            choice_set["Brand"].map(brand_utils) +
            choice_set["Ads"].map(ads_utils) +
            choice_set["Price"].apply(price_util)
        )
        # Add Gumbel noise
        choice_set["Error"] = -np.log(-np.log(np.random.rand(n_alts)))
        choice_set["U_total"] = choice_set["Utility"] + choice_set["Error"]
        choice_set["Chosen"] = (choice_set["U_total"] == choice_set["U_total"].max()).astype(int)
        responses.append(choice_set)
    return pd.concat(responses)

# Simulate data for all respondents
data = pd.concat([simulate_responder(i) for i in range(1, 101)], ignore_index=True)

# Keep only observable columns
df = data[["Resp", "Task", "Brand", "Ads", "Price", "Chosen"]]
df.head()
```





## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.



```{python}
# One-hot encode features
df_prepared = pd.get_dummies(df, columns=["Brand", "Ads"], drop_first=True)

# Create model matrix
X_cols = ['Brand_Netflix', 'Brand_Prime', 'Ads_Yes', 'Price']
X = df_prepared[X_cols].astype(np.float64).values
y = df_prepared["Chosen"].astype(int).values
group_ids = df_prepared["Resp"].astype(int).astype("category").cat.codes * 10 + df_prepared["Task"] - 1
group = group_ids.values  # unique task IDs
```



## 4. Estimation via Maximum Likelihood



```{python}

from scipy.special import logsumexp

# Log-likelihood using grouped softmax
def neg_log_likelihood(beta):
    utilities = X @ beta
    log_like = 0
    for g in np.unique(group):
        idx = group == g
        u = utilities[idx]
        y_group = y[idx]
        log_probs = u - logsumexp(u)
        log_like += np.sum(y_group * log_probs)
    return -log_like  # negative for minimization
```






```{python}
from scipy.optimize import minimize

# Initial guess
beta_init = np.zeros(X.shape[1])

# Estimate MLE
result = minimize(neg_log_likelihood, beta_init, method='BFGS')
beta_hat = result.x
vcov = result.hess_inv
se = np.sqrt(np.diag(vcov))
z = 1.96

# 95% Confidence intervals
ci_bounds = np.vstack((beta_hat - z * se, beta_hat + z * se)).T

# Final results table
param_labels = ['beta_Netflix', 'beta_Prime', 'beta_Ads', 'beta_Price']
results = pd.DataFrame({
    "Parameter": param_labels,
    "Estimate": beta_hat,
    "Std. Error": se,
    "CI Lower": ci_bounds[:, 0],
    "CI Upper": ci_bounds[:, 1]
})
results
```

The estimated parameters from the multinomial logit model represent the influence of each product attribute on a consumer's likelihood of choosing a particular option.

As shown in the table above:

- **Positive coefficients** (e.g., for Brand_P and Brand_H) indicate that those attributes increase the utility of the product, making it more likely to be chosen.
- **Negative coefficients** (e.g., for Ad_Yes and Price) suggest those features reduce product attractiveness and decrease the probability of choice.

The **price coefficient** being negative aligns with economic theory â€” higher prices discourage selection. The **negative ad coefficient** reflects a common preference for ad-free content. In contrast, **positive brand coefficients** imply that consumers value more familiar or premium brands over the baseline option.

These results offer interpretable, statistically significant insights into how each attribute contributes to product choice, and they serve as a baseline for comparison with the Bayesian estimates in the next section.




## 5. Estimation via Bayesian Methods


```{python}
import numpy as np

# Prior: N(0, 5) for binary vars, N(0,1) for price
prior_sd = np.array([5.0, 5.0, 5.0, 1.0])  # [brand_P, brand_H, ad_Yes, price]

def log_prior(beta):
    return -0.5 * np.sum((beta / prior_sd)**2)

def log_likelihood(beta):
    utilities = X @ beta
    log_like = 0
    for g in np.unique(group):
        idx = group == g
        u = utilities[idx]
        y_g = y[idx]
        log_probs = u - logsumexp(u)
        log_like += np.sum(y_g * log_probs)
    return log_like

def log_posterior(beta):
    return log_likelihood(beta) + log_prior(beta)

# Metropolis-Hastings Sampler
n_iter = 11000
burn_in = 1000
beta_samples = np.zeros((n_iter, X.shape[1]))
current = np.zeros(X.shape[1])

for i in range(n_iter):
    proposal = current + np.random.normal(0, 0.1, size=current.shape)
    log_accept_ratio = log_posterior(proposal) - log_posterior(current)
    if np.log(np.random.rand()) < log_accept_ratio:
        current = proposal
    beta_samples[i] = current

# Remove burn-in
beta_post = beta_samples[burn_in:]
```



```{python}
import matplotlib.pyplot as plt

# Select beta_Price (last column, index = 3)
price_samples = beta_post[:, 3]

# Plot: Trace plot
plt.figure(figsize=(12, 4))
plt.plot(price_samples, lw=0.5)
plt.title("Trace Plot of beta_Price")
plt.xlabel("Iteration")
plt.ylabel("beta_Price")
plt.grid(True)
plt.show()

# Plot: Histogram of posterior distribution
plt.figure(figsize=(8, 4))
plt.hist(price_samples, bins=50, density=True, alpha=0.7)
plt.title("Posterior Distribution of beta_Price")
plt.xlabel("beta_Price")
plt.ylabel("Density")
plt.grid(True)
plt.show()
```




```{python}
# Compute posterior summaries
posterior_means = beta_post.mean(axis=0)
posterior_sds = beta_post.std(axis=0)
cred_int_95 = np.percentile(beta_post, [2.5, 97.5], axis=0).T

# Create results table
params = ['beta_brand_P', 'beta_brand_H', 'beta_ad_Yes', 'beta_price']
bayes_df = pd.DataFrame({
    "Parameter": params,
    "Posterior Mean": posterior_means,
    "Posterior Std. Dev.": posterior_sds,
    "95% Credible Lower": cred_int_95[:, 0],
    "95% Credible Upper": cred_int_95[:, 1]
})

bayes_df
```

## Interpretation of MLE Results

The table of maximum likelihood estimates (MLEs) provides insight into how different product attributes influence consumer choice.

- **Brand_P** and **Brand_H** both have positive coefficients, indicating that these brands are more preferred compared to the reference brand (likely Hulu, since it's not explicitly shown as a variable). Among the two, the higher value for Brand_P suggests it is the most favored.
- **Ad_Yes** has a negative coefficient, confirming that the presence of advertisements reduces the likelihood of a product being selected.
- **Price** has a negative coefficient, as expected, showing that higher prices decrease the utility of the product and reduce the probability of choice.

The standard errors are relatively small, and all 95% confidence intervals exclude zero, suggesting the estimates are statistically significant. The signs and relative magnitudes of the coefficients align with consumer intuition: people prefer ad-free, affordable options from more popular brands.

In summary, the MNL model confirms expected consumer preferences and yields interpretable results that help quantify how much each feature matters in driving choices.


# Discussion 

If we had not simulated the data ourselves, we would interpret the estimated parameters as insights drawn from real consumer preferences. In this case, the model suggests that Netflix is the most preferred brand among the three options, followed by Prime Video, with Hulu serving as the baseline (i.e., the least preferred).

The fact that $\beta_\text{Netflix} > \beta_\text{Prime}$ tells us that, all else equal, consumers derive more utility from Netflix than from Prime. This could reflect stronger brand loyalty, better content perception, or a more favorable user experience associated with Netflix.

The negative sign on $\beta_\text{price}$ aligns well with consumer economic behavior â€” as price increases, the likelihood of choosing a product decreases. This is consistent with standard theory: price acts as a deterrent when consumers are making trade-offs between otherwise similar options.

Overall, the direction and magnitude of the coefficients not only make sense but also reflect patterns we would expect to see in real-world streaming service choice behavior. The model successfully captures key drivers of utility and provides interpretable, data-driven insights.



To simulate data from a hierarchical (multi-level) logit model, we would need to allow each respondent to have their own set of preference parameters (Î²). Instead of assigning a single global Î² vector to everyone, we would draw each individual's Î² from a population-level distribution â€” typically a multivariate normal distribution with its own mean vector and covariance matrix.

This results in whatâ€™s called a **random-parameters logit model**, where individual heterogeneity is explicitly modeled. In simulation, for each respondent $i$, we would generate:

\[
\beta_i \sim \mathcal{N}(\mu, \Sigma)
\]

And use $\beta_i$ to compute utilities and simulate choices for that individual.

To estimate such a model, we would need to use a Bayesian method like **Metropolis-within-Gibbs**, **Hamiltonian Monte Carlo**, or **variational inference**, since the model includes latent parameters at two levels:
- The individual-level betas ($\beta_i$)
- The population-level hyperparameters ($\mu, \Sigma$)

Such models are much more flexible and better suited to **real-world conjoint data**, where preferences naturally vary across individuals. This structure captures both the overall market trend and personalized utility drivers at the respondent level.












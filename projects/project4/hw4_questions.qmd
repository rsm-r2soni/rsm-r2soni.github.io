---
title: "Add Title"
author: "Rishabh Soni"
date: today
---

_todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._


## 1a. K-Means

_todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working.  Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables.  Compare your results to the built-in `kmeans` function in R or Python._

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# Load and clean the dataset
penguins = pd.read_csv("palmer_penguins.csv")
df = penguins.dropna(subset=['bill_length_mm', 'flipper_length_mm'])

# Extract and scale features
X = df[['bill_length_mm', 'flipper_length_mm']].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

```{python}
# Helper functions for K-means
def euclidean(a, b):
    return np.sqrt(np.sum((a - b)**2))

def assign_clusters(X, centroids):
    return np.array([np.argmin([euclidean(x, c) for c in centroids]) for x in X])

def compute_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def kmeans_with_plots(X, k, max_iters=5):
    np.random.seed(42)
    centroids = X[np.random.choice(len(X), k, replace=False)]

    for step in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = compute_centroids(X, labels, k)

        # Plot the current step
        plt.figure(figsize=(6, 5))
        sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette='Set1')
        plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X', s=200, label='Centroids')
        plt.title(f"K-Means Step {step + 1}")
        plt.xlabel("Bill Length (scaled)")
        plt.ylabel("Flipper Length (scaled)")
        plt.legend()
        plt.show()

        if np.allclose(centroids, new_centroids):
            print("Converged.")
            break
        centroids = new_centroids

    return labels, centroids
```

```{python}
# Run the algorithm with 3 clusters
labels, centroids = kmeans_with_plots(X_scaled, k=3)
```

```{python}
# Compare with sklearn's KMeans
from sklearn.cluster import KMeans

model = KMeans(n_clusters=3, random_state=42)
model.fit(X_scaled)
labels_sklearn = model.labels_

# Plot comparison
plt.figure(figsize=(6, 5))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels_sklearn, palette='Set2')
plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], c='black', marker='X', s=200)
plt.title("Sklearn KMeans Result")
plt.xlabel("Bill Length (scaled)")
plt.ylabel("Flipper Length (scaled)")
plt.legend()
plt.show()
```


_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). What is the "right" number of clusters as suggested by these two metrics?_


```{python}
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Initialize result containers
wss = []  # within-cluster sum of squares
silhouette_scores = []
K_range = range(2, 8)  # K from 2 to 7

# Loop through K values
for k in K_range:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(X_scaled)
    wss.append(model.inertia_)  # inertia_ is the WSS
    silhouette_scores.append(silhouette_score(X_scaled, model.labels_))
```

```{python}
# Plot WSS (Elbow Method) and Silhouette Score
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# Elbow Method
ax[0].plot(K_range, wss, marker='o')
ax[0].set_title("Elbow Method (WSS)")
ax[0].set_xlabel("Number of Clusters (K)")
ax[0].set_ylabel("Within-Cluster Sum of Squares")

# Silhouette Score
ax[1].plot(K_range, silhouette_scores, marker='o', color='green')
ax[1].set_title("Silhouette Scores")
ax[1].set_xlabel("Number of Clusters (K)")
ax[1].set_ylabel("Silhouette Score")

plt.tight_layout()
plt.show()
```


_If you want a challenge, add your plots as an animated gif on your website so that the result looks something like [this](https://www.youtube.com/shorts/XCsoWZU9oN8)._

### K-Means Animation with Colorful Clusters


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import imageio
from sklearn.preprocessing import StandardScaler

# Load and clean data
df = pd.read_csv("palmer_penguins.csv").dropna(subset=['bill_length_mm', 'flipper_length_mm'])
X = df[['bill_length_mm', 'flipper_length_mm']].values
X_scaled = StandardScaler().fit_transform(X)

# Output folder
gif_dir = "gif_frames"
os.makedirs(gif_dir, exist_ok=True)

# Helper functions
def euclidean(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

def assign_clusters(X, centroids):
    return np.array([np.argmin([euclidean(x, c) for c in centroids]) for x in X])

def compute_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

# Main animation function
def animate_kmeans(X, k=3, max_iters=10):
    np.random.seed(42)
    centroids = X[np.random.choice(len(X), k, replace=False)]

    for step in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = compute_centroids(X, labels, k)

        plt.figure(figsize=(6, 6))
        for i in range(k):
            cluster_points = X[labels == i]
            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=10, alpha=0.6)

        plt.scatter(centroids[:, 0], centroids[:, 1], c='none', edgecolor='black', s=300, marker='o', linewidths=2, label='Centroids')
        plt.title(f"K-Means Step {step + 1}")
        plt.xticks([]); plt.yticks([])
        plt.tight_layout()
        plt.savefig(f"{gif_dir}/frame_{step:02d}.png", dpi=100)
        plt.close()

        if np.allclose(centroids, new_centroids):
            print(f"Converged at step {step + 1}")
            break

        centroids = new_centroids

# Run the function
animate_kmeans(X_scaled, k=3)
```

---

###  Create the GIF

```{python}
# Combine saved frames into a GIF
images = []
frame_files = sorted([f for f in os.listdir(gif_dir) if f.endswith(".png")])

for file in frame_files:
    img = imageio.imread(os.path.join(gif_dir, file))
    images.append(img)

gif_path = "kmeans_penguins.gif"
imageio.mimsave(gif_path, images, duration=0.8)
print("GIF created at:", gif_path)
```



## 1b. Latent-Class MNL

_todo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57._

_The data provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current "wide" format into a "long" format._

_todo: Fit the standard MNL model on these data.  Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes._

_todo: How many classes are suggested by the $BIC = -2*\ell_n  + k*log(n)$? (where $\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) The Bayesian-Schwarz Information Criterion [link](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate -- akin to the adjusted R-squared for the linear regression model. Note, that a **lower** BIC indicates a better model fit, accounting for the number of parameters in the model._

_todo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC._



## 2a. K Nearest Neighbors

_todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function._


_todo: plot the data where the horizontal axis is `x1`, the vertical axis is `x2`, and the points are colored by the value of `y`.  You may optionally draw the wiggly boundary._

_todo: generate a test dataset with 100 points, using the same code as above but with a different seed._

_todo: implement KNN by hand.  Check you work with a built-in function -- eg, `class::knn()` or `caret::train(method="knn")` in R, or scikit-learn's `KNeighborsClassifier` in Python._

_todo: run your function for k=1,...,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?_ 



## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

### 2b. Key Drivers Analysis â€“ Load and Prepare Data

```{python}
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load data
df = pd.read_csv("data_for_drivers_analysis.csv")

# Select features and target
X = df[['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']]
y = df['satisfaction']

# Standardize predictors for regression
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
```


_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._

## 2b. Key Drivers Analysis

We analyze the drivers of `satisfaction` using multiple techniques. This includes:
- Pearson correlations
- Standardized linear regression coefficients
- Random Forest feature importances

```{python}
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

# Load the data
df = pd.read_csv("data_for_drivers_analysis.csv")

# Define predictors and target
X = df[['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']]
y = df['satisfaction']

# 1. Pearson Correlations
pearson_corr = X.corrwith(y)

# 2. Standardized Linear Regression Coefficients
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
linreg = LinearRegression().fit(X_scaled, y)
std_coefs = pd.Series(linreg.coef_, index=X.columns)

# 3. Random Forest Gini Importance
rf = RandomForestRegressor(random_state=42)
rf.fit(X, y)
rf_importance = pd.Series(rf.feature_importances_, index=X.columns)

# Combine all results
results_df = pd.DataFrame({
    'Pearson Correlation': pearson_corr,
    'Standardized Coefficient': std_coefs,
    'Random Forest Gini Importance': rf_importance
}).round(3).sort_values("Pearson Correlation", ascending=False)

results_df
```





